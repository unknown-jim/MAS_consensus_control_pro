"""
è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡åž‹
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

from config import (
    DEVICE, NUM_FOLLOWERS, MAX_STEPS, SAVE_MODEL_PATH,
    STATE_DIM, HIDDEN_DIM, ACTION_DIM, NUM_AGENTS,
    USE_NEIGHBOR_INFO, MAX_NEIGHBORS, set_seed, SEED
)
from topology import DirectedSpanningTreeTopology
from environment import LeaderFollowerMASEnv
from networks import DecentralizedActor


class ModelEvaluator:
    """æ¨¡åž‹è¯„ä¼°å™¨"""
    
    def __init__(self, model_path=SAVE_MODEL_PATH, use_fixed_seed=True, enable_randomization=False):
        """
        Args:
            model_path: æ¨¡åž‹è·¯å¾„
            use_fixed_seed: æ˜¯å¦ä½¿ç”¨å›ºå®šç§å­
            enable_randomization: æ˜¯å¦å¯ç”¨é¢†å¯¼è€…è½¨è¿¹éšæœºåŒ–
                - False: ä½¿ç”¨å›ºå®šè½¨è¿¹ï¼ˆä¸Žè®­ç»ƒä¸­ collect_trajectory è¯„ä¼°ä¸€è‡´ï¼‰
                - True: éšæœºè½¨è¿¹ï¼ˆæµ‹è¯•æ³›åŒ–æ€§ï¼‰
        """
        self.model_path = model_path
        self.use_fixed_seed = use_fixed_seed
        self.enable_randomization = enable_randomization
        
        # å›ºå®šç§å­ä»¥å¤çŽ°è®­ç»ƒæ—¶çš„çŽ¯å¢ƒ
        if use_fixed_seed:
            set_seed(SEED)
            print(f"ðŸŽ² ä½¿ç”¨å›ºå®šéšæœºç§å­: {SEED}")
        
        self.topology = DirectedSpanningTreeTopology(NUM_FOLLOWERS)
        # å…³é”®ï¼šenable_randomization æŽ§åˆ¶é¢†å¯¼è€…è½¨è¿¹æ˜¯å¦éšæœºåŒ–
        self.env = LeaderFollowerMASEnv(self.topology, enable_randomization=enable_randomization)
        
        if enable_randomization:
            print(f"ðŸŽ² é¢†å¯¼è€…è½¨è¿¹: éšæœºåŒ– (æµ‹è¯•æ³›åŒ–æ€§)")
        else:
            print(f"ðŸ“Œ é¢†å¯¼è€…è½¨è¿¹: å›ºå®š (å¤çŽ°è®­ç»ƒè¯„ä¼°çŽ¯å¢ƒ)")
        
        # åŠ è½½æ¨¡åž‹
        self.actor = self._load_model()
        
        # é¢„è®¡ç®—é‚»å±…ä¿¡æ¯
        if USE_NEIGHBOR_INFO:
            self._precompute_neighbor_info()
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹"""
        actor = DecentralizedActor(
            STATE_DIM, HIDDEN_DIM,
            use_neighbor_info=USE_NEIGHBOR_INFO
        ).to(DEVICE)
        
        checkpoint = torch.load(self.model_path, map_location=DEVICE)
        actor.load_state_dict(checkpoint['actor'])
        actor.eval()
        
        print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ: {self.model_path}")
        print(f"   è®­ç»ƒ Episode: {checkpoint.get('episode', 'N/A')}")
        reward = checkpoint.get('reward', 'N/A')
        if isinstance(reward, (int, float)):
            print(f"   æœ€ä½³å¥–åŠ±: {reward:.2f}")
        else:
            print(f"   æœ€ä½³å¥–åŠ±: {reward}")
        
        return actor
    
    def _precompute_neighbor_info(self):
        """é¢„è®¡ç®—é‚»å±…ç´¢å¼•"""
        self.neighbor_indices = {}
        for follower_id in range(1, NUM_AGENTS):
            neighbors = self.topology.get_neighbors(follower_id)
            self.neighbor_indices[follower_id] = neighbors[:MAX_NEIGHBORS]
    
    def _get_neighbor_obs(self, states):
        """èŽ·å–é‚»å±…è§‚æµ‹"""
        if not USE_NEIGHBOR_INFO:
            return None
        
        neighbor_obs_list = []
        for follower_id in range(1, NUM_AGENTS):
            neighbors = self.neighbor_indices.get(follower_id, [])
            if len(neighbors) > 0:
                neighbor_states = states[neighbors]
                if len(neighbors) < MAX_NEIGHBORS:
                    padding = torch.zeros(MAX_NEIGHBORS - len(neighbors), STATE_DIM, device=DEVICE)
                    neighbor_states = torch.cat([neighbor_states, padding], dim=0)
            else:
                neighbor_states = torch.zeros(MAX_NEIGHBORS, STATE_DIM, device=DEVICE)
            neighbor_obs_list.append(neighbor_states)
        
        return torch.stack(neighbor_obs_list, dim=0)
    
    @torch.no_grad()
    def select_action(self, states, deterministic=True):
        """é€‰æ‹©åŠ¨ä½œ"""
        follower_states = states[1:]  # æŽ’é™¤é¢†å¯¼è€…
        neighbor_obs = self._get_neighbor_obs(states)
        
        if deterministic:
            actions, _ = self.actor(follower_states, neighbor_obs, deterministic=True)
        else:
            actions, _ = self.actor(follower_states, neighbor_obs, deterministic=False)
        
        return actions
    
    def run_episode(self, deterministic=True, render=False, seed=None):
        """è¿è¡Œä¸€ä¸ª episode"""
        # å¯é€‰ï¼šä¸ºæ¯ä¸ª episode è®¾ç½®ä¸åŒç§å­
        if seed is not None:
            set_seed(seed)
        
        state = self.env.reset()
        
        # è®°å½•æ•°æ®
        rewards = []
        errors = []
        comm_rates = []
        leader_positions = []
        follower_positions = []
        leader_velocities = []
        follower_velocities = []
        
        for step in range(MAX_STEPS):
            action = self.select_action(state, deterministic=deterministic)
            next_state, reward, done, info = self.env.step(action)
            
            # å¤„ç† reward å¯èƒ½æ˜¯ tensor æˆ– float
            if hasattr(reward, 'mean'):
                rewards.append(reward.mean().item())
            else:
                rewards.append(float(reward))
            errors.append(info['tracking_error'])
            comm_rates.append(info['comm_rate'])
            
            leader_positions.append(self.env.positions[0].item())
            follower_positions.append(self.env.positions[1:].cpu().numpy())
            leader_velocities.append(self.env.velocities[0].item())
            follower_velocities.append(self.env.velocities[1:].cpu().numpy())
            
            state = next_state
            
            if done:
                break
        
        return {
            'total_reward': sum(rewards),
            'avg_error': np.mean(errors),
            'final_error': errors[-1],
            'avg_comm_rate': np.mean(comm_rates),
            'leader_pos': np.array(leader_positions),
            'follower_pos': np.array(follower_positions),
            'leader_vel': np.array(leader_velocities),
            'follower_vel': np.array(follower_velocities),
            'errors': np.array(errors),
            'comm_rates': np.array(comm_rates)
        }
    
    def evaluate(self, num_episodes=10, deterministic=True, use_different_seeds=False):
        """è¯„ä¼°æ¨¡åž‹æ€§èƒ½
        
        Args:
            num_episodes: è¯„ä¼°çš„ episode æ•°é‡
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            use_different_seeds: æ˜¯å¦ä¸ºæ¯ä¸ª episode ä½¿ç”¨ä¸åŒç§å­
        """
        print(f"\n{'='*60}")
        print(f"ðŸ” æ¨¡åž‹è¯„ä¼° (å…± {num_episodes} ä¸ª episodes)")
        if self.enable_randomization:
            print(f"   é¢†å¯¼è€…è½¨è¿¹: éšæœºåŒ–")
        else:
            print(f"   é¢†å¯¼è€…è½¨è¿¹: å›ºå®š")
        if use_different_seeds:
            print(f"   éšæœºç§å­: ä¸åŒ")
        else:
            print(f"   éšæœºç§å­: å›ºå®š (SEED={SEED})")
        print(f"{'='*60}")
        
        all_rewards = []
        all_errors = []
        all_comm_rates = []
        
        for ep in range(num_episodes):
            # ä¸ºæ¯ä¸ª episode è®¾ç½®ç§å­
            if use_different_seeds:
                seed = SEED + ep  # ä¸åŒç§å­æµ‹è¯•æ³›åŒ–
            else:
                seed = SEED  # å›ºå®šç§å­å¤çŽ°
            
            set_seed(seed)
            result = self.run_episode(deterministic=deterministic)
            all_rewards.append(result['total_reward'])
            all_errors.append(result['avg_error'])
            all_comm_rates.append(result['avg_comm_rate'])
            
            print(f"  Episode {ep+1:2d} | R: {result['total_reward']:7.1f} | "
                  f"Err: {result['avg_error']:.4f} | Comm: {result['avg_comm_rate']*100:.1f}%")
        
        print(f"\n{'='*60}")
        print(f"ðŸ“Š è¯„ä¼°ç»“æžœæ±‡æ€»:")
        print(f"{'='*60}")
        print(f"  å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.2f} Â± {np.std(all_rewards):.2f}")
        print(f"  å¹³å‡è¯¯å·®: {np.mean(all_errors):.4f} Â± {np.std(all_errors):.4f}")
        print(f"  å¹³å‡é€šä¿¡çŽ‡: {np.mean(all_comm_rates)*100:.1f}% Â± {np.std(all_comm_rates)*100:.1f}%")
        print(f"  æœ€ä½³å¥–åŠ±: {max(all_rewards):.2f}")
        print(f"  æœ€å°è¯¯å·®: {min(all_errors):.4f}")
        print(f"{'='*60}")
        
        return {
            'mean_reward': np.mean(all_rewards),
            'std_reward': np.std(all_rewards),
            'mean_error': np.mean(all_errors),
            'std_error': np.std(all_errors),
            'mean_comm': np.mean(all_comm_rates),
            'std_comm': np.std(all_comm_rates)
        }
    
    def visualize(self, save_path='evaluation_result.png'):
        """å¯è§†åŒ–ä¸€ä¸ª episode çš„ç»“æžœ"""
        result = self.run_episode(deterministic=True)
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'Model Evaluation (R={result["total_reward"]:.1f}, Err={result["avg_error"]:.4f})', 
                     fontsize=14, fontweight='bold')
        
        time = np.arange(len(result['leader_pos'])) * 0.05  # DT=0.05
        
        # 1. ä½ç½®è·Ÿè¸ª
        ax1 = axes[0, 0]
        ax1.plot(time, result['leader_pos'], 'r-', linewidth=2, label='Leader')
        follower_pos = result['follower_pos']
        for i in range(follower_pos.shape[1]):
            ax1.plot(time, follower_pos[:, i], '--', alpha=0.5, linewidth=1)
        ax1.plot(time, follower_pos.mean(axis=1), 'b-', linewidth=2, label='Avg Follower')
        ax1.set_xlabel('Time (s)')
        ax1.set_ylabel('Position')
        ax1.set_title('Position Tracking')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. é€Ÿåº¦è·Ÿè¸ª
        ax2 = axes[0, 1]
        ax2.plot(time, result['leader_vel'], 'r-', linewidth=2, label='Leader')
        follower_vel = result['follower_vel']
        ax2.plot(time, follower_vel.mean(axis=1), 'b-', linewidth=2, label='Avg Follower')
        ax2.set_xlabel('Time (s)')
        ax2.set_ylabel('Velocity')
        ax2.set_title('Velocity Tracking')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è·Ÿè¸ªè¯¯å·®
        ax3 = axes[1, 0]
        ax3.plot(time, result['errors'], 'g-', linewidth=1.5)
        ax3.axhline(y=result['avg_error'], color='r', linestyle='--', label=f'Avg: {result["avg_error"]:.4f}')
        ax3.set_xlabel('Time (s)')
        ax3.set_ylabel('Tracking Error')
        ax3.set_title('Tracking Error over Time')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. é€šä¿¡çŽ‡
        ax4 = axes[1, 1]
        ax4.plot(time, result['comm_rates'] * 100, 'purple', linewidth=1.5)
        ax4.axhline(y=result['avg_comm_rate']*100, color='r', linestyle='--', 
                    label=f'Avg: {result["avg_comm_rate"]*100:.1f}%')
        ax4.set_xlabel('Time (s)')
        ax4.set_ylabel('Communication Rate (%)')
        ax4.set_title('Communication Rate over Time')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"\nðŸ“ˆ å¯è§†åŒ–ç»“æžœå·²ä¿å­˜: {save_path}")
        
        return result


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(SAVE_MODEL_PATH).exists():
        print(f"âŒ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {SAVE_MODEL_PATH}")
        return
    
    # ============================================================
    # 1. å›ºå®šè½¨è¿¹è¯„ä¼°ï¼ˆå¤çŽ°è®­ç»ƒä¸­çš„è¯„ä¼°çŽ¯å¢ƒï¼‰
    # ============================================================
    print("\n" + "="*60)
    print("ðŸ“Œ å›ºå®šè½¨è¿¹è¯„ä¼° (å¤çŽ°è®­ç»ƒè¯„ä¼°çŽ¯å¢ƒ)")
    print("   - é¢†å¯¼è€…è½¨è¿¹: å›ºå®š (sine, A=2.0, Ï‰=0.5)")
    print("   - éšæœºç§å­: å›ºå®š")
    print("="*60)
    
    evaluator_fixed = ModelEvaluator(
        use_fixed_seed=True, 
        enable_randomization=False  # å…³é—­éšæœºåŒ–
    )
    stats_fixed = evaluator_fixed.evaluate(num_episodes=5, deterministic=True, use_different_seeds=False)
    
    # å¯è§†åŒ–å›ºå®šè½¨è¿¹ç»“æžœ
    set_seed(SEED)
    evaluator_fixed.visualize(save_path='evaluation_fixed.png')
    
    # ============================================================
    # 2. éšæœºè½¨è¿¹è¯„ä¼°ï¼ˆæµ‹è¯•æ³›åŒ–æ€§ï¼‰
    # ============================================================
    print("\n" + "="*60)
    print("ðŸŽ² éšæœºè½¨è¿¹è¯„ä¼° (æµ‹è¯•æ³›åŒ–æ€§)")
    print("   - é¢†å¯¼è€…è½¨è¿¹: éšæœºåŒ–")
    print("   - éšæœºç§å­: ä¸åŒ")
    print("="*60)
    
    evaluator_random = ModelEvaluator(
        use_fixed_seed=False,
        enable_randomization=True  # å¼€å¯éšæœºåŒ–
    )
    stats_random = evaluator_random.evaluate(num_episodes=5, deterministic=True, use_different_seeds=True)
    
    # å¯è§†åŒ–éšæœºè½¨è¿¹ç»“æžœ
    evaluator_random.visualize(save_path='evaluation_random.png')
    
    # ============================================================
    # å¯¹æ¯”æ€»ç»“
    # ============================================================
    print("\n" + "="*60)
    print("ðŸ“Š è¯„ä¼°å¯¹æ¯”æ€»ç»“")
    print("="*60)
    print(f"  å›ºå®šè½¨è¿¹: R={stats_fixed['mean_reward']:.2f}Â±{stats_fixed['std_reward']:.2f}, "
          f"Err={stats_fixed['mean_error']:.4f}")
    print(f"  éšæœºè½¨è¿¹: R={stats_random['mean_reward']:.2f}Â±{stats_random['std_reward']:.2f}, "
          f"Err={stats_random['mean_error']:.4f}")
    print("="*60)


if __name__ == '__main__':
    main()
