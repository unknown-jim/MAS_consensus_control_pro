"""
é…ç½®æ–‡ä»¶ - CTDE æ¶æ„ç‰ˆï¼ˆä¿®å¤ç­–ç•¥å´©æºƒï¼‰
"""
import torch
import random
import numpy as np

# ==================== è®¾å¤‡é…ç½® ====================
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SEED = 42
TOPOLOGY_SEED = 42

# ==================== ç½‘ç»œæ‹“æ‰‘ ====================
NUM_FOLLOWERS = 6
NUM_PINNED = 2
NUM_AGENTS = NUM_FOLLOWERS + 1

# ==================== éšæœºåŒ–é…ç½® ====================
ENABLE_RANDOMIZATION = True

# é¢†å¯¼è€…åŠ¨åŠ›å­¦éšæœºåŒ–èŒƒå›´
LEADER_AMP_RANGE = (1.0, 3.0)
LEADER_OMEGA_RANGE = (0.3, 0.8)
LEADER_PHASE_RANGE = (0.0, 2 * np.pi)
LEADER_TRAJECTORY_TYPES = ['sine', 'cosine', 'mixed', 'chirp']

# è·Ÿéšè€…çŠ¶æ€éšæœºåŒ–èŒƒå›´
FOLLOWER_POS_INIT_STD_RANGE = (0.1, 0.5)
FOLLOWER_VEL_INIT_STD_RANGE = (0.05, 0.2)

# ==================== ç¯å¢ƒå‚æ•° ====================
STATE_DIM = 4
ACTION_DIM = 2
DT = 0.05
MAX_STEPS = 300  # âœ… å‡å°‘æ­¥æ•°ï¼ŒåŠ å¿«è¿­ä»£

LEADER_AMPLITUDE = 2.0
LEADER_OMEGA = 0.5
LEADER_PHASE = 0.0

POS_LIMIT = 10.0
VEL_LIMIT = 10.0

# ==================== é€šä¿¡å‚æ•° ====================
THRESHOLD_MIN = 0.00
THRESHOLD_MAX = 1.0

# ==================== å¥–åŠ±å‚æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰====================
REWARD_MIN = -2.0
REWARD_MAX = 2.0
USE_SOFT_REWARD_SCALING = False  # âœ… å…³é—­è½¯ç¼©æ”¾ï¼Œä½¿ç”¨ç¡¬è£å‰ª

# ---------- è·Ÿè¸ªæƒ©ç½šå‚æ•° ----------
TRACKING_ERROR_SCALE = 1.5  # âœ… é™ä½æ•æ„Ÿåº¦
TRACKING_PENALTY_MAX = 1.0  # âœ… é™ä½æœ€å¤§æƒ©ç½š

# ---------- é€šä¿¡æƒ©ç½šå‚æ•° ----------
COMM_PENALTY_BASE = 0.05  # âœ… å¢åŠ é€šä¿¡æˆæœ¬
COMM_WEIGHT_DECAY = 1.0   # âœ… å‡ç¼“é€šä¿¡ä¼˜åŒ–å‹åŠ›ï¼ˆåŸ2.0ï¼‰

# ---------- æ”¹è¿›å¥–åŠ±å‚æ•° ----------
IMPROVEMENT_SCALE = 0.5   # âœ… å¤§å¹…é™ä½æ”¹è¿›å¥–åŠ±æƒé‡
IMPROVEMENT_CLIP = 0.1    # âœ… æ›´ä¸¥æ ¼çš„è£å‰ª

# ==================== SAC å‚æ•°ï¼ˆæ–¹æ¡ˆä¸‰ï¼šç¨³å®šåŸºç¡€ä¸Šæå‡æ€§èƒ½ï¼‰====================
LEARNING_RATE = 8e-5      # âœ… ç•¥å¾®æé«˜å­¦ä¹ ç‡ï¼ˆåŸ5e-5ï¼‰
ALPHA_LR = 3e-5           # âœ… ä¿æŒä½ alpha å­¦ä¹ ç‡
GAMMA = 0.99
TAU = 0.005
INIT_ALPHA = 0.6          # âœ… å‡å°‘æ¢ç´¢åŠ é€Ÿæ”¶æ•›ï¼ˆåŸ0.8ï¼‰
AUTO_ALPHA = True
TARGET_ENTROPY_SCALE = 1.0  # âœ… æ–°å¢ï¼šç›®æ ‡ç†µç¼©æ”¾å› å­
HIDDEN_DIM = 256
BUFFER_SIZE = 500000
BATCH_SIZE = 256          # âœ… å‡å°æ‰¹æ¬¡å¤§å°
GRADIENT_STEPS = 1        # âœ… å‡å°‘æ¢¯åº¦æ­¥æ•°

# ==================== ç½‘ç»œå‚æ•° ====================
LOG_STD_MIN = -20
LOG_STD_MAX = 2
U_SCALE = 5.0
TH_SCALE = 1.0

# ==================== è®­ç»ƒå‚æ•°ï¼ˆæ–¹æ¡ˆä¸‰ï¼šå»¶é•¿è®­ç»ƒï¼‰====================
NUM_EPISODES = 600        # âœ… å»¶é•¿è®­ç»ƒè§‚å¯Ÿæ”¶æ•›ï¼ˆåŸ450ï¼‰
VIS_INTERVAL = 10
NUM_PARALLEL_ENVS = 16    # âœ… å‡å°‘å¹¶è¡Œç¯å¢ƒæ•°
UPDATE_FREQUENCY = 8      # âœ… é™ä½æ›´æ–°é¢‘ç‡
WARMUP_STEPS = 5000       # âœ… æ–°å¢ï¼šé¢„çƒ­æ­¥æ•°
USE_AMP = True
SAVE_MODEL_PATH = 'best_model.pt'

# ==================== CTDE å‚æ•° ====================
USE_CTDE = True
USE_NEIGHBOR_INFO = True
NEIGHBOR_AGGREGATION = 'attention'
MAX_NEIGHBORS = 5
ACTOR_NUM_LAYERS = 3
CRITIC_NUM_LAYERS = 3
USE_ATTENTION_CRITIC = False


def set_seed(seed=SEED):
    """è®¾ç½®å…¨å±€éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


def print_config():
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("=" * 60)
    print("ğŸ”§ Configuration - CTDE with Adaptive Reward (Fixed)")
    print("=" * 60)
    print(f"  Device: {DEVICE}")
    print(f"  Seed: {SEED}")
    print()
    print("ğŸ“Š Multi-Agent System:")
    print(f"  Agents: {NUM_AGENTS} (1 Leader + {NUM_FOLLOWERS} Followers)")
    print(f"  Pinned Followers: {NUM_PINNED}")
    print(f"  State Dim: {STATE_DIM}, Action Dim: {ACTION_DIM}")
    print()
    print("ğŸ² Randomization:")
    print(f"  Enabled: {ENABLE_RANDOMIZATION}")
    if ENABLE_RANDOMIZATION:
        print(f"  Leader Amplitude Range: {LEADER_AMP_RANGE}")
        print(f"  Leader Omega Range: {LEADER_OMEGA_RANGE}")
        print(f"  Trajectory Types: {LEADER_TRAJECTORY_TYPES}")
    print()
    print("ğŸ® Environment:")
    print(f"  Max Steps: {MAX_STEPS}, DT: {DT}s")
    print(f"  Position Limit: Â±{POS_LIMIT}, Velocity Limit: Â±{VEL_LIMIT}")
    print()
    print("ğŸ Reward Design (Fixed):")
    print(f"  Soft Scaling: {USE_SOFT_REWARD_SCALING}")
    print(f"  Tracking: -tanh(error * {TRACKING_ERROR_SCALE}) * {TRACKING_PENALTY_MAX}")
    print(f"  Comm Penalty Base: {COMM_PENALTY_BASE}")
    print(f"  Improvement Scale: {IMPROVEMENT_SCALE}, Clip: Â±{IMPROVEMENT_CLIP}")
    print()
    print("ğŸ“¡ Communication:")
    print(f"  Threshold Range: [{THRESHOLD_MIN}, {THRESHOLD_MAX}]")
    print()
    print("ğŸ—ï¸ CTDE Architecture:")
    print(f"  Actor Uses Neighbor Info: {USE_NEIGHBOR_INFO}")
    if USE_NEIGHBOR_INFO:
        print(f"  Neighbor Aggregation: {NEIGHBOR_AGGREGATION}")
    print()
    print("ğŸ§  SAC Parameters (Fixed):")
    print(f"  Learning Rate: {LEARNING_RATE}, Alpha LR: {ALPHA_LR}")
    print(f"  Init Alpha: {INIT_ALPHA}")
    print(f"  Gamma: {GAMMA}, Tau: {TAU}")
    print(f"  Hidden Dim: {HIDDEN_DIM}")
    print()
    print("ğŸš€ Training (Fixed):")
    print(f"  Episodes: {NUM_EPISODES}, Parallel Envs: {NUM_PARALLEL_ENVS}")
    print(f"  Batch Size: {BATCH_SIZE}, Buffer Size: {BUFFER_SIZE}")
    print(f"  Update Frequency: {UPDATE_FREQUENCY}, Warmup Steps: {WARMUP_STEPS}")
    print(f"  Use AMP: {USE_AMP}")
    print("=" * 60)


def get_config_dict():
    """è·å–é…ç½®å­—å…¸"""
    return {
        # è®¾å¤‡å’Œç§å­
        'device': str(DEVICE),
        'seed': SEED,
        'topology_seed': TOPOLOGY_SEED,
        
        # ç½‘ç»œæ‹“æ‰‘
        'num_followers': NUM_FOLLOWERS,
        'num_pinned': NUM_PINNED,
        'num_agents': NUM_AGENTS,
        
        # éšæœºåŒ–é…ç½®
        'enable_randomization': ENABLE_RANDOMIZATION,
        'leader_amp_range': LEADER_AMP_RANGE,
        'leader_omega_range': LEADER_OMEGA_RANGE,
        'leader_phase_range': LEADER_PHASE_RANGE,
        'leader_trajectory_types': LEADER_TRAJECTORY_TYPES,
        'follower_pos_init_std_range': FOLLOWER_POS_INIT_STD_RANGE,
        'follower_vel_init_std_range': FOLLOWER_VEL_INIT_STD_RANGE,
        
        # ç¯å¢ƒå‚æ•°
        'state_dim': STATE_DIM,
        'action_dim': ACTION_DIM,
        'dt': DT,
        'max_steps': MAX_STEPS,
        'pos_limit': POS_LIMIT,
        'vel_limit': VEL_LIMIT,
        
        # å¥–åŠ±å‚æ•°
        'reward_min': REWARD_MIN,
        'reward_max': REWARD_MAX,
        'use_soft_reward_scaling': USE_SOFT_REWARD_SCALING,
        'tracking_error_scale': TRACKING_ERROR_SCALE,
        'tracking_penalty_max': TRACKING_PENALTY_MAX,
        'comm_penalty_base': COMM_PENALTY_BASE,
        'comm_weight_decay': COMM_WEIGHT_DECAY,
        'improvement_scale': IMPROVEMENT_SCALE,
        'improvement_clip': IMPROVEMENT_CLIP,
        
        # é€šä¿¡å‚æ•°
        'threshold_min': THRESHOLD_MIN,
        'threshold_max': THRESHOLD_MAX,
        
        # SAC å‚æ•°
        'learning_rate': LEARNING_RATE,
        'alpha_lr': ALPHA_LR,
        'gamma': GAMMA,
        'tau': TAU,
        'init_alpha': INIT_ALPHA,
        'auto_alpha': AUTO_ALPHA,
        'hidden_dim': HIDDEN_DIM,
        'buffer_size': BUFFER_SIZE,
        'batch_size': BATCH_SIZE,
        'gradient_steps': GRADIENT_STEPS,
        
        # ç½‘ç»œå‚æ•°
        'log_std_min': LOG_STD_MIN,
        'log_std_max': LOG_STD_MAX,
        'u_scale': U_SCALE,
        'th_scale': TH_SCALE,
        
        # è®­ç»ƒå‚æ•°
        'num_episodes': NUM_EPISODES,
        'vis_interval': VIS_INTERVAL,
        'num_parallel_envs': NUM_PARALLEL_ENVS,
        'update_frequency': UPDATE_FREQUENCY,
        'warmup_steps': WARMUP_STEPS,
        'use_amp': USE_AMP,
        
        # CTDE å‚æ•°
        'use_ctde': USE_CTDE,
        'use_neighbor_info': USE_NEIGHBOR_INFO,
        'neighbor_aggregation': NEIGHBOR_AGGREGATION,
        'max_neighbors': MAX_NEIGHBORS,
        'actor_num_layers': ACTOR_NUM_LAYERS,
        'critic_num_layers': CRITIC_NUM_LAYERS,
    }