"""
é¢†å¯¼è€…-è·Ÿéšè€…å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç¯å¢ƒ - è¯¾ç¨‹å­¦ä¹ ä¼˜åŒ–ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰
"""
import torch
import math

from config import (
    DEVICE, STATE_DIM, DT,
    COMM_PENALTY_INIT, 
    THRESHOLD_MIN_INIT, THRESHOLD_MAX_INIT,
    THRESHOLD_MIN_FINAL, THRESHOLD_MAX_FINAL,
    COMM_BONUS_INIT,
    LEADER_AMPLITUDE, LEADER_OMEGA, LEADER_PHASE,
    POS_LIMIT, VEL_LIMIT,
    REWARD_MIN, REWARD_MAX, USE_SOFT_REWARD_SCALING,
    TH_SCALE  # ğŸ”§ æ·»åŠ å¯¼å…¥
)


class BatchedLeaderFollowerEnv:
    """å®Œå…¨å‘é‡åŒ–çš„æ‰¹é‡ç¯å¢ƒ - è¯¾ç¨‹å­¦ä¹ ä¼˜åŒ–ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰"""
    
    def __init__(self, topology, num_envs=64):
        self.topology = topology
        self.num_envs = num_envs
        self.num_agents = topology.num_agents
        self.num_followers = topology.num_followers
        self.leader_id = topology.leader_id
        
        self.leader_amplitude = LEADER_AMPLITUDE
        self.leader_omega = LEADER_OMEGA
        self.leader_phase = LEADER_PHASE
        
        self.pos_limit = POS_LIMIT
        self.vel_limit = VEL_LIMIT
        self.reward_min = REWARD_MIN
        self.reward_max = REWARD_MAX
        self.use_soft_scaling = USE_SOFT_REWARD_SCALING
        
        # æ§åˆ¶å™¨å¢ç›Š
        self.base_pos_gain = 5.0
        self.base_vel_gain = 2.5
        
        # ğŸ”§ è¯¾ç¨‹å­¦ä¹ å‚æ•°ï¼ˆç”±å¤–éƒ¨è®¾ç½®ï¼‰
        self.comm_penalty = COMM_PENALTY_INIT
        self.threshold_min = THRESHOLD_MIN_INIT
        self.threshold_max = THRESHOLD_MAX_INIT
        self.comm_bonus = COMM_BONUS_INIT
        
        # ğŸ”§ è¯¾ç¨‹å­¦ä¹ è¿›åº¦ï¼ˆ0-1ï¼‰
        self.curriculum_progress = 0.0
        
        # ğŸ”§ é˜ˆå€¼ç¼©æ”¾å› å­ï¼ˆä¸ networks.py ä¿æŒä¸€è‡´ï¼‰
        self.th_scale = TH_SCALE
        
        self.role_ids = torch.zeros(self.num_agents, dtype=torch.long, device=DEVICE)
        self.role_ids[1:] = 1
        
        self._precompute_neighbor_info()
        
        # é¢„åˆ†é…çŠ¶æ€å¼ é‡
        self.positions = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.velocities = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.last_broadcast_pos = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.last_broadcast_vel = torch.zeros(num_envs, self.num_agents, device=DEVICE)
        self.t = torch.zeros(num_envs, device=DEVICE)
        
        self._prev_error = None
        self.reset()
    
    def set_curriculum_params(self, comm_penalty, threshold_min, threshold_max, comm_bonus, progress):
        """
        è®¾ç½®è¯¾ç¨‹å­¦ä¹ å‚æ•°
        
        Args:
            comm_penalty: é€šä¿¡æƒ©ç½šç³»æ•°
            threshold_min: é˜ˆå€¼ä¸‹ç•Œ
            threshold_max: é˜ˆå€¼ä¸Šç•Œ
            comm_bonus: é€šä¿¡å¥–åŠ±ç³»æ•°
            progress: è¯¾ç¨‹è¿›åº¦ [0, 1]
        """
        self.comm_penalty = comm_penalty
        self.threshold_min = threshold_min
        self.threshold_max = threshold_max
        self.comm_bonus = comm_bonus
        self.curriculum_progress = progress
    
    def set_comm_penalty(self, penalty):
        """è®¾ç½®å½“å‰é€šä¿¡æƒ©ç½šç³»æ•°ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        self.comm_penalty = penalty
    
    def _precompute_neighbor_info(self):
        """é¢„è®¡ç®—é‚»å±…èšåˆçŸ©é˜µ"""
        self.adj_matrix = torch.zeros(self.num_agents, self.num_agents, device=DEVICE)
        edge_index = self.topology.edge_index
        
        for i in range(edge_index.shape[1]):
            src, dst = edge_index[0, i].item(), edge_index[1, i].item()
            self.adj_matrix[dst, src] = 1.0
        
        in_degree = self.adj_matrix.sum(dim=1)
        self.degree_matrix = torch.diag(in_degree)
        self.laplacian = self.degree_matrix - self.adj_matrix
        
        in_degree_safe = in_degree.clamp(min=1.0)
        self.norm_adj_matrix = self.adj_matrix / in_degree_safe.unsqueeze(1)
        
        self.pinning_gains = torch.zeros(self.num_agents, device=DEVICE)
        for f in self.topology.pinned_followers:
            self.pinning_gains[f] = 2.0
    
    def _leader_state_batch(self, t):
        """æ‰¹é‡è®¡ç®—é¢†å¯¼è€…çŠ¶æ€"""
        pos = self.leader_amplitude * torch.sin(self.leader_omega * t + self.leader_phase)
        vel = self.leader_amplitude * self.leader_omega * torch.cos(self.leader_omega * t + self.leader_phase)
        return pos, vel
    
    def reset(self, env_ids=None):
        """é‡ç½®ç¯å¢ƒ"""
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        
        num_reset = len(env_ids) if isinstance(env_ids, torch.Tensor) else self.num_envs
        
        self.t[env_ids] = 0.0
        
        leader_pos, leader_vel = self._leader_state_batch(self.t[env_ids])
        
        self.positions[env_ids, 0] = leader_pos
        self.velocities[env_ids, 0] = leader_vel
        
        # ğŸ”§ åˆå§‹ä½ç½®æ ¹æ®è¯¾ç¨‹è¿›åº¦è°ƒæ•´
        init_pos_std = 0.2 + 0.3 * self.curriculum_progress
        init_vel_std = 0.05 + 0.1 * self.curriculum_progress
        
        self.positions[env_ids, 1:] = leader_pos.unsqueeze(1) + torch.randn(
            num_reset, self.num_followers, device=DEVICE
        ) * init_pos_std
        self.velocities[env_ids, 1:] = leader_vel.unsqueeze(1) + torch.randn(
            num_reset, self.num_followers, device=DEVICE
        ) * init_vel_std
        
        self.last_broadcast_pos[env_ids] = self.positions[env_ids].clone()
        self.last_broadcast_vel[env_ids] = self.velocities[env_ids].clone()
        
        self._prev_error = None
        
        return self._get_state()
    
    def _get_state(self):
        """æ„å»ºè§‚æµ‹çŠ¶æ€"""
        state = torch.zeros(self.num_envs, self.num_agents, STATE_DIM, device=DEVICE)
        
        neighbor_avg_pos = torch.matmul(self.last_broadcast_pos, self.norm_adj_matrix.T)
        neighbor_avg_vel = torch.matmul(self.last_broadcast_vel, self.norm_adj_matrix.T)
        
        pos_error = self.positions - neighbor_avg_pos
        vel_error = self.velocities - neighbor_avg_vel
        
        state[:, :, 0] = pos_error / (self.pos_limit + 1e-6)
        state[:, :, 1] = vel_error / (self.vel_limit + 1e-6)
        state[:, :, 2] = self.positions / (self.pos_limit + 1e-6)
        state[:, :, 3] = self.velocities / (self.vel_limit + 1e-6)
        
        return state
    
    def _compute_base_control(self):
        """è®¡ç®—åŸºç¡€ä¸€è‡´æ€§æ§åˆ¶"""
        follower_pos = self.last_broadcast_pos[:, 1:]
        follower_vel = self.last_broadcast_vel[:, 1:]
        
        leader_pos = self.last_broadcast_pos[:, 0:1]
        leader_vel = self.last_broadcast_vel[:, 0:1]
        
        follower_adj = self.adj_matrix[1:, 1:]
        follower_degree = follower_adj.sum(dim=1, keepdim=True).clamp(min=1.0)
        
        neighbor_pos_sum = torch.matmul(follower_pos, follower_adj.T)
        neighbor_vel_sum = torch.matmul(follower_vel, follower_adj.T)
        
        pos_consensus_error = follower_pos * follower_degree.T - neighbor_pos_sum
        vel_consensus_error = follower_vel * follower_degree.T - neighbor_vel_sum
        
        pinning_gains_followers = self.pinning_gains[1:]
        pos_pinning_error = (follower_pos - leader_pos) * pinning_gains_followers
        vel_pinning_error = (follower_vel - leader_vel) * pinning_gains_followers
        
        base_control = (
            -self.base_pos_gain * (pos_consensus_error + pos_pinning_error)
            -self.base_vel_gain * (vel_consensus_error + vel_pinning_error)
        )
        
        return base_control
    
    def _scale_reward_batch(self, reward):
        """æ‰¹é‡å¥–åŠ±ç¼©æ”¾"""
        if self.use_soft_scaling:
            mid = (self.reward_max + self.reward_min) / 2
            scale = (self.reward_max - self.reward_min) / 2
            normalized = (reward - mid) / (scale + 1e-8)
            return mid + scale * torch.tanh(normalized)
        else:
            return torch.clamp(reward, self.reward_min, self.reward_max)
    
    def step(self, action):
        """æ‰¹é‡æ‰§è¡Œä¸€æ­¥"""
        self.t += DT
        
        # æ›´æ–°é¢†å¯¼è€…
        leader_pos, leader_vel = self._leader_state_batch(self.t)
        self.positions[:, 0] = leader_pos
        self.velocities[:, 0] = leader_vel
        
        # è§£æåŠ¨ä½œ
        delta_u = action[:, :, 0] * 2.0
        raw_threshold = action[:, :, 1]
        
        # ğŸ”§ ä¿®å¤ï¼šçº¿æ€§æ˜ å°„ï¼Œä¸å†ä½¿ç”¨ sigmoid
        # raw_threshold èŒƒå›´æ˜¯ [0, TH_SCALE]ï¼Œç›´æ¥å½’ä¸€åŒ–åˆ° [0, 1]
        normalized_threshold = raw_threshold / self.th_scale
        # ç¡®ä¿å½’ä¸€åŒ–å€¼åœ¨ [0, 1] èŒƒå›´å†…
        normalized_threshold = normalized_threshold.clamp(0.0, 1.0)
        # æ˜ å°„åˆ°å½“å‰è¯¾ç¨‹é˜¶æ®µçš„é˜ˆå€¼èŒƒå›´
        threshold = self.threshold_min + (self.threshold_max - self.threshold_min) * normalized_threshold
        threshold = threshold.clamp(min=0.001, max=0.5)
        
        # è®¡ç®—æ€»æ§åˆ¶
        base_u = self._compute_base_control()
        total_u = base_u + delta_u
        total_u = torch.clamp(total_u, -20.0, 20.0)
        
        # è·Ÿéšè€…åŠ¨åŠ›å­¦
        follower_pos = self.positions[:, 1:]
        follower_vel = self.velocities[:, 1:]
        
        nonlinear_term = 0.2 * torch.sin(follower_pos) - 0.1 * follower_vel
        acc = total_u + nonlinear_term
        
        new_vel = torch.clamp(follower_vel + acc * DT, -self.vel_limit, self.vel_limit)
        new_pos = torch.clamp(follower_pos + new_vel * DT, -self.pos_limit, self.pos_limit)
        
        self.positions[:, 1:] = new_pos
        self.velocities[:, 1:] = new_vel
        
        # äº‹ä»¶è§¦å‘é€šä¿¡
        trigger_error = torch.abs(new_pos - self.last_broadcast_pos[:, 1:])
        is_triggered = trigger_error > threshold
        
        self.last_broadcast_pos[:, 1:] = torch.where(
            is_triggered, self.positions[:, 1:], self.last_broadcast_pos[:, 1:]
        )
        self.last_broadcast_vel[:, 1:] = torch.where(
            is_triggered, self.velocities[:, 1:], self.last_broadcast_vel[:, 1:]
        )
        self.last_broadcast_pos[:, 0] = self.positions[:, 0]
        self.last_broadcast_vel[:, 0] = self.velocities[:, 0]
        
        # ==================== è®¡ç®—å¥–åŠ±ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================
        pos_error = torch.abs(self.positions[:, 1:] - self.positions[:, 0:1])
        vel_error = torch.abs(self.velocities[:, 1:] - self.velocities[:, 0:1])
        
        tracking_error = pos_error.mean(dim=1) + 0.5 * vel_error.mean(dim=1)
        
        # è·Ÿè¸ªå¥–åŠ±ï¼ˆä¸»è¦å¥–åŠ±ï¼‰
        tracking_reward = torch.exp(-tracking_error) * 2.5 - 1.0
        
        # æ”¹è¿›å¥–åŠ±
        improvement_bonus = torch.zeros_like(tracking_error)
        if self._prev_error is not None:
            improvement = self._prev_error - tracking_error
            improvement_bonus = torch.clamp(improvement * 3.0, -0.5, 0.5)
        self._prev_error = tracking_error.detach().clone()
        
        # é€šä¿¡ç‡
        comm_rate = is_triggered.float().mean(dim=1)
        
        # é€šä¿¡æƒ©ç½š
        comm_penalty = comm_rate * self.comm_penalty
        
        # é€šä¿¡å¥–åŠ±ï¼ˆæ—©æœŸé˜¶æ®µï¼‰
        comm_reward = comm_rate * self.comm_bonus
        
        # ä¸€è‡´æ€§å¥–åŠ±
        max_error = pos_error.max(dim=1)[0]
        consensus_bonus = torch.where(
            max_error < 0.3,
            torch.ones_like(max_error) * 0.2,
            torch.zeros_like(max_error)
        )
        
        # æ€»å¥–åŠ±
        raw_reward = (
            tracking_reward +
            improvement_bonus +
            comm_reward -
            comm_penalty +
            consensus_bonus
        )
        rewards = self._scale_reward_batch(raw_reward)
        
        dones = torch.zeros(self.num_envs, dtype=torch.bool, device=DEVICE)
        
        infos = {
            'tracking_error': tracking_error,
            'comm_rate': comm_rate,
            'leader_pos': self.positions[:, 0],
            'leader_vel': self.velocities[:, 0],
            'avg_follower_pos': self.positions[:, 1:].mean(dim=1),
            'threshold_mean': threshold.mean(),
            'threshold_min': self.threshold_min,
            'threshold_max': self.threshold_max,
            'comm_penalty_coef': self.comm_penalty,
            'comm_bonus_coef': self.comm_bonus,
            'curriculum_progress': self.curriculum_progress,
            'tracking_reward': tracking_reward.mean(),
            'comm_reward': comm_reward.mean(),
            'comm_penalty_value': comm_penalty.mean(),
        }
        
        return self._get_state(), rewards, dones, infos


class LeaderFollowerMASEnv:
    """å•ç¯å¢ƒç‰ˆæœ¬"""
    
    def __init__(self, topology):
        self.batched_env = BatchedLeaderFollowerEnv(topology, num_envs=1)
        self.topology = topology
        self.num_agents = topology.num_agents
        self.num_followers = topology.num_followers
        self.role_ids = self.batched_env.role_ids
    
    @property
    def positions(self):
        return self.batched_env.positions[0]
    
    @property
    def velocities(self):
        return self.batched_env.velocities[0]
    
    @property
    def t(self):
        return self.batched_env.t[0].item()
    
    def set_curriculum_params(self, comm_penalty, threshold_min, threshold_max, comm_bonus, progress):
        """è®¾ç½®è¯¾ç¨‹å­¦ä¹ å‚æ•°"""
        self.batched_env.set_curriculum_params(
            comm_penalty, threshold_min, threshold_max, comm_bonus, progress
        )
    
    def set_comm_penalty(self, penalty):
        """å…¼å®¹æ—§æ¥å£"""
        self.batched_env.set_comm_penalty(penalty)
    
    def reset(self):
        state = self.batched_env.reset()
        return state[0]
    
    def step(self, action):
        action_batched = action.unsqueeze(0)
        states, rewards, dones, infos = self.batched_env.step(action_batched)
        info = {k: (v[0].item() if isinstance(v, torch.Tensor) and v.dim() > 0 else 
                    v.item() if isinstance(v, torch.Tensor) else v) 
                for k, v in infos.items()}
        return states[0], rewards[0].item(), dones[0].item(), info